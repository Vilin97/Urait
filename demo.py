# app.py
import streamlit as st
import pandas as pd
import src.utils as utils

st.set_page_config(page_title="–ü–æ–¥–±–æ—Ä –∫—É—Ä—Å–∞ –ø–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ", layout="centered")

YES_TOKENS = {"–¥–∞", "yes", "true", "1"}
NO_TOKENS  = {"–Ω–µ—Ç", "no", "false", "0"}

def to_bool_ru(x: object) -> bool:
    s = str(x).strip().lower()
    if s in YES_TOKENS: return True
    if s in NO_TOKENS:  return False
    return False  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî ¬´–Ω–µ—Ç¬ª

DEFAULT_PROMPT = (
    "–ò–∑–≤–ª–µ–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã/–∫—É—Ä—Å–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –≤ –Ω—ë–º –∏–∑—É—á–∞—é—Ç—Å—è, "
    "–≤—Å—ë –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –í–∫–ª—é—á–∞–π —Ç–æ–ª—å–∫–æ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã, –Ω–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ. "
    "–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–º–∏ –∑–∞–ø—è—Ç—ã–º–∏. –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã/–∫—É—Ä—Å–∞ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–µ—Ä–≤—ã–º."
)

@st.cache_resource
def get_client():
    return utils.get_api_client()

@st.cache_resource
def load_courses_and_embeddings():
    courses_df = pd.read_csv("courses.csv")
    embeddings_by_id = utils.load_course_embeddings()
    return courses_df, embeddings_by_id

def run_matching(url: str, parse_prompt: str, top_k: int = 5):
    client = get_client()

    # –®–∞–≥ 1: –ø–∞—Ä—Å–∏–Ω–≥
    st.info("üîç –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç‚Ä¶")
    parsed_topics = utils.parse_document(url, parse_prompt, client)
    toks = [s.strip() for s in parsed_topics.split(",") if s.strip()]
    if not toks:
        raise ValueError("–ü–∞—Ä—Å–∏–Ω–≥ –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
    discipline, topics = toks[0], toks[1:]
    st.success("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω.")
    with st.container(border=True):
        st.markdown("**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞:**")
        st.markdown(f"‚Ä¢ **–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** {discipline}")
        if topics:
            st.markdown("‚Ä¢ **–¢–µ–º—ã:** " + ", ".join(topics))

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    text_to_embed = f"{discipline}, {', '.join(topics)}"
    embedding = utils.embed_text(text_to_embed, client)

    # –®–∞–≥ 2: –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –∫—É—Ä—Å–æ–≤
    courses_df, embeddings_by_id = load_courses_and_embeddings()
    st.info(f"üìä –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-{top_k} –±–ª–∏–∂–∞–π—à–∏—Ö –∫—É—Ä—Å–æ–≤‚Ä¶")
    most_similar = utils.get_most_similar(embedding, embeddings_by_id, top_k=top_k)  # [(id, score)]
    id2score = dict(most_similar)
    top_ids = [cid for cid, _ in most_similar]
    st.success(f"‚úÖ –¢–æ–ø-{top_k} –Ω–∞–π–¥–µ–Ω.")
    with st.container(border=True):
        st.markdown("**–ù–∞–π–¥–µ–Ω–Ω—ã–µ –±–ª–∏–∂–∞–π—à–∏–µ –∫—É—Ä—Å—ã (ID ‚Üí —Å—Ö–æ–¥—Å—Ç–≤–æ):**")
        st.write([{"ID –∫—É—Ä—Å–∞": cid, "–°—Ö–æ–¥—Å—Ç–≤–æ": float(f"{id2score[cid]:.4f}")} for cid in top_ids])

    # –¢–∞–±–ª–∏—Ü–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–ø–æ–∫–∞ –±–µ–∑ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏)
    sim_df = (
        courses_df[courses_df["project_id"].isin(top_ids)].copy()
        .assign(similarity=lambda df: df["project_id"].map(id2score).astype(float))
        .sort_values("similarity", ascending=False)
        .reset_index(drop=True)
    )

    # –®–∞–≥ 3: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ ‚Äî —Å –ø–æ—ç—Ç–∞–ø–Ω—ã–º –≤—ã–≤–æ–¥–æ–º –∏ –¥–æ—Å—Ä–æ—á–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
    st.info("ü§ñ –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç—å –∫—É—Ä—Å–æ–≤‚Ä¶")
    log_box = st.container()  # —Å—é–¥–∞ –±—É–¥–µ–º –ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É
    processed_rows = []
    early_stop = False

    for i, row in sim_df.iterrows():
        course_id = row["project_id"]
        course_name = row["project_name"]
        course_topics = row["topics"]

        with log_box.expander(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{i+1}: {course_name} (ID {course_id})", expanded=True if i == 0 else False):
            st.caption(f"–¢–µ–º—ã –∫—É—Ä—Å–∞: {course_topics}")
            raw_decision, explanation = utils.determine_course_suitability(
                discipline, topics, course_name, course_topics, client
            )
            suitable = to_bool_ru(raw_decision)
            st.markdown(f"**–†–µ—à–µ–Ω–∏–µ:** {'‚úÖ –î–∞' if suitable else '‚ùå –ù–µ—Ç'}")
            st.markdown(f"**–ü–æ—è—Å–Ω–µ–Ω–∏–µ:** {explanation}")

        processed_rows.append(
            {
                "project_id": course_id,
                "project_name": course_name,
                "topics": course_topics,
                "similarity": float(row["similarity"]),
                "–ü—Ä–∏–≥–æ–¥–µ–Ω": bool(suitable),
                "–ü–æ—è—Å–Ω–µ–Ω–∏–µ": explanation,
            }
        )

        if suitable:
            st.success("üõë –ù–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫—É—Ä—Å ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –æ—Å—Ç–∞–ª—å–Ω—ã—Ö.")
            early_stop = True
            break

    st.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞." if not early_stop else "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–æ—Å—Ä–æ—á–Ω–æ.")

    # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π DataFrame —Ç–æ–ª—å–∫–æ –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    out_df = pd.DataFrame(processed_rows)
    out_df = out_df.rename(columns={
        "project_id":   "ID –∫—É—Ä—Å–∞",
        "project_name": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞",
        "similarity":   "–°—Ö–æ–¥—Å—Ç–≤–æ",
        "topics":       "–¢–µ–º—ã –∫—É—Ä—Å–∞",
    })
    out_df = out_df[["ID –∫—É—Ä—Å–∞", "–ù–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞", "–¢–µ–º—ã –∫—É—Ä—Å–∞", "–°—Ö–æ–¥—Å—Ç–≤–æ", "–ü—Ä–∏–≥–æ–¥–µ–Ω", "–ü–æ—è—Å–Ω–µ–Ω–∏–µ"]]
    # –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É (–Ω–∞ —Å–ª—É—á–∞–π –¥–æ—Å—Ä–æ—á–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ –ø–æ –ø–µ—Ä–≤–æ–º—É)
    out_df = out_df.sort_values("–°—Ö–æ–¥—Å—Ç–≤–æ", ascending=False).reset_index(drop=True)

    return discipline, topics, out_df

# ===== UI =====
st.title("–ü–æ–¥–±–æ—Ä –∫—É—Ä—Å–∞ –ø–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ")
url = st.text_input("–í–≤–µ–¥–∏—Ç–µ URL –†–ü–î (PDF –∏–ª–∏ HTML)")

col1, col2 = st.columns([1, 1])
with col1:
    top_k = st.number_input("–°–∫–æ–ª—å–∫–æ –∫—É—Ä—Å–æ–≤ –∏—Å–∫–∞—Ç—å (top-k)?", min_value=1, max_value=50, value=5, step=1)
with col2:
    with st.expander("–ü—Ä–æ–º–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"):
        parse_prompt = st.text_area("–ü—Ä–æ–º–ø—Ç", value=DEFAULT_PROMPT, height=120)

if st.button("–ù–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫—É—Ä—Å—ã") and url:
    with st.spinner("–ò–¥—ë—Ç –ø–∞—Ä—Å–∏–Ω–≥, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫‚Ä¶"):
        try:
            discipline, topics, sim_df = run_matching(url, parse_prompt, top_k=top_k)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞: {e}")
        else:
            st.subheader("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞")
            st.markdown(f"**–î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞:** {discipline}")
            if topics:
                st.markdown("**–¢–µ–º—ã:** " + ", ".join(topics))

            st.subheader(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (–¥–æ {top_k})")
            st.dataframe(
                sim_df,  # –∫–æ–ª–æ–Ω–∫–∏: ID –∫—É—Ä—Å–∞, –ù–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞, –¢–µ–º—ã –∫—É—Ä—Å–∞, –°—Ö–æ–¥—Å—Ç–≤–æ, –ü—Ä–∏–≥–æ–¥–µ–Ω, –ü–æ—è—Å–Ω–µ–Ω–∏–µ
                use_container_width=True,
            )

            # –ò—Ç–æ–≥–æ–≤—ã–π –≤–µ—Ä–¥–∏–∫—Ç
            yes = sim_df[sim_df["–ü—Ä–∏–≥–æ–¥–µ–Ω"]]
            st.subheader("–í–µ—Ä–¥–∏–∫—Ç")
            if len(yes):
                best = yes.sort_values("–°—Ö–æ–¥—Å—Ç–≤–æ", ascending=False).iloc[0]
                st.success(f"‚úÖ –ü—Ä–∏–≥–æ–¥–µ–Ω: {best['–ù–∞–∑–≤–∞–Ω–∏–µ –∫—É—Ä—Å–∞']} (–æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ {best['–°—Ö–æ–¥—Å—Ç–≤–æ']:.3f})")
                st.caption(best["–ü–æ—è—Å–Ω–µ–Ω–∏–µ"])
            else:
                st.warning("‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫—É—Ä—Å–æ–≤ —Å—Ä–µ–¥–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
